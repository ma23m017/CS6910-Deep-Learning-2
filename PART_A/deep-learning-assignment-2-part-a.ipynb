{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7999112,"sourceType":"datasetVersion","datasetId":4710248}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing essential libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Hyper-parameters\nnum_epochs = 6\nbatch_size = 32\nlearning_rate = 0.001\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to a uniform size\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n])\n\n# Load dataset\ndataset = ImageFolder(root='/kaggle/input/i-naturalist1/inaturalist_12K/train', transform=transform)\n\n# Split dataset into training and validation sets\ntraining_data_size = int(0.8 * len(dataset))\nvalidation_data_size = len(dataset) - training_data_size\ntraining_dataset, validation_dataset = random_split(dataset, [training_data_size, validation_data_size])\n\n# Create data loaders\ntraining_data_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\nvalidation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n#classes = dataset.classes\n#print(classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining class for CNN\nclass FlexibleCNN(nn.Module):\n    def __init__(self, num_classes, in_channels=3, conv_filters=[16, 32, 64, 128, 256], \n                 kernel_sizes=[3, 3, 3, 3, 3], pool_sizes=[2, 2, 2, 2, 2], \n                 dense_units=512, activation='ReLU', batch_norm = True, dropout = 0.0):\n        self.conv_filters = conv_filters\n        self.kernel_sizes = kernel_sizes\n        self.pool_sizes = pool_sizes\n        self.dense_units = dense_units\n        self.activation = activation\n        self.batch_norm = batch_norm\n        self.dropout = dropout\n        \n        # Invoking the constructor of the base class\n        super(FlexibleCNN, self).__init__()\n\n        # Convolutional layers\n        self.conv_layers = nn.ModuleList()\n        prev_out_size = 224\n        in_channels = in_channels\n        for out_channels, kernel_size, pool_size in zip(conv_filters, kernel_sizes, pool_sizes):\n            conv_layer = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size),\n                self.get_activation(activation),\n                nn.MaxPool2d(pool_size)\n            )\n            \n            # Making condtion to add the batch norm and dropout\n            if batch_norm:\n                conv_layer.add_module(f\"batch_norm_{out_channels}\", nn.BatchNorm2d(out_channels))\n            if dropout > 0.0:\n                conv_layer.add_module(f\"dropout_{out_channels}\", nn.Dropout2d(dropout))\n            self.conv_layers.append(conv_layer)\n            in_channels = out_channels\n            \n        # Defining function to calculate and return the kernel size dynamically\n        def cal_size(stride, padding, kernel_size, prev_size):\n            new_size = (prev_size-kernel_size+2*padding)/stride + 1\n            return new_size//2\n        \n        prev_size = 224 # Starting with the size 224\n        \n        # Running the loop five times for 5 number of blocks\n        for i in range(5):\n            new_size1 = cal_size(stride = 1, padding = 0, kernel_size = self.kernel_sizes[i], prev_size= prev_size)\n            prev_size = new_size1\n        print(new_size1)\n\n        #fully connected layers\n        #self.fc1 = nn.Linear(conv_filters[-1] * 7 * 7, dense_units)\n        self.fc1 = nn.Linear(self.conv_filters[4] * int(new_size1) * int(new_size1), self.dense_units)\n        self.fc2 = nn.Linear(self.dense_units, 10)#num_classes)\n\n    def forward(self, x):\n        # Forward pass through convolutional layers\n        for conv_layer in self.conv_layers:\n            x = conv_layer(x)\n\n        # Flatten the output for the fully connected layer\n        x = torch.flatten(x, 1)\n\n        # Forward pass through fully connected layers\n        #x = F.relu(self.fc1(x))\n        x = self.get_activation(self.activation)(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \n    # Defining function to get the different activation functions\n    def get_activation(self, activation):\n        if activation == 'ReLU':\n            return nn.ReLU()\n        elif activation == 'GELU':\n            return nn.GELU()\n        elif activation == 'SiLU':\n            return nn.SiLU()\n        elif activation == 'Mish':\n            return nn.Mish()\n        else:\n            raise ValueError(\"Invalid activation function\")\n            \n#model = FlexibleCNN(10)\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:45:01.032154Z","iopub.execute_input":"2024-04-07T19:45:01.032556Z","iopub.status.idle":"2024-04-07T19:45:01.049465Z","shell.execute_reply.started":"2024-04-07T19:45:01.032526Z","shell.execute_reply":"2024-04-07T19:45:01.048437Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install wandb  #installing wandb","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:45:05.351894Z","iopub.execute_input":"2024-04-07T19:45:05.352538Z","iopub.status.idle":"2024-04-07T19:45:17.789307Z","shell.execute_reply.started":"2024-04-07T19:45:05.352507Z","shell.execute_reply":"2024-04-07T19:45:17.788098Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.4)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.42.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing wandb and essential libraries\nimport wandb\nimport numpy as np\nfrom types import SimpleNamespace\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:45:25.648238Z","iopub.execute_input":"2024-04-07T19:45:25.648631Z","iopub.status.idle":"2024-04-07T19:45:25.654051Z","shell.execute_reply.started":"2024-04-07T19:45:25.648603Z","shell.execute_reply":"2024-04-07T19:45:25.652882Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"wandb.login(key='cd7a6c2259e8886dc269bbf6f0f9e55089d3beeb')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:45:35.111127Z","iopub.execute_input":"2024-04-07T19:45:35.111809Z","iopub.status.idle":"2024-04-07T19:45:35.212767Z","shell.execute_reply.started":"2024-04-07T19:45:35.111774Z","shell.execute_reply":"2024-04-07T19:45:35.211811Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Defining sweep configuration\nsweep_config = {\n    'method': 'bayes',\n    'name' : 'sweep test23',\n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'kernel_size':{\n            'values': [[3,3,3,3,3], [3,5,5,7,7], [7,7,5,5,3]]\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n        'activation': {\n            'values': ['ReLU', 'GELU', 'SiLU', 'Mish']\n        },\n        'batch_norm':{\n            'values': [True,False]\n        },\n        'filt_org':{\n            'values': [[32,32,32,32,32],[128,128,64,64,32],[32,64,128,256,512]]\n        },\n        'data_augment': {\n            'values': [False]\n        },\n        'num_dense':{\n            'values': [128, 256]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep=sweep_config, project='Deep_Learning_Assignment2')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    '''\n    WandB calls main function each time with differnet combination.\n\n    We can retrive the same and use the same values for our hypermeters.\n\n    '''\n\n\n    with wandb.init(entity = 'prabhat-kumar') as run:\n\n        run_name=\"-act_\"+wandb.config.activation+\"-ks\"+str(wandb.config.kernel_size)+'-nd'+str(wandb.config.num_dense)\n        wandb.run.name=run_name\n#         actv = get_activation(wandb.config.activation)\n        model = FlexibleCNN(num_classes=10, in_channels=3, conv_filters=wandb.config.filt_org, \n                 kernel_sizes=wandb.config.kernel_size, pool_sizes=[2, 2, 2, 2, 2], \n                 dense_units=wandb.config.num_dense, activation=wandb.config.activation).to(device)\n        \n        \n        # Loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   # Using Adam as an optimizer\n\n        # Training loop\n        for epoch in range(num_epochs):\n            #initializing the necessary values\n            total_training_data = 0\n            correct_predicted_train_data = 0\n            loss1 = 0\n            # Train the model\n            model.train()\n            for images, labels in training_data_loader:\n                # Moving images and labels to GPU\n                images = images.to(device)\n                labels = labels.to(device)\n        \n                # Forward pass\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n        \n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                loss1 = loss1 + loss.item()\n                \n                # Calculate the training accuracy\n                max_value, predicted_by_model = torch.max(outputs, 1)\n                total_training_data = total_training_data + labels.size(0)\n                correct_predicted_train_data = correct_predicted_train_data + (predicted_by_model == labels).sum().item()\n                \n            train_accuracy = 100*correct_predicted_train_data/total_training_data\n            print(f'Epoch number {epoch+1}, Training Accuracy: {train_accuracy:.2f}%') # Printing training accuracy\n            wandb.log({'train_loss': round(loss1 / len(training_data_loader), 2)})\n            wandb.log({'train_accuracy': round(train_accuracy, 2)})\n\n            #wandb.log({'train_loss':running_loss/len(train_loader)})\n            #wandb.log({'train_accuracy':train_accuracy})\n            \n        \n    \n            # Validate the model\n            model.eval()\n            with torch.no_grad():\n                # Initializing the values\n                correct_predicted_validation_data = 0\n                total_validation_data = 0\n                for images, labels in validation_data_loader:\n                    \n                    images = images.to(device)  # Moving images to GPU\n                    labels = labels.to(device)  # Moving labels to GPU\n            \n                    # Forward pass\n                    outputs = model(images)\n                    max_value, predicted_by_model = torch.max(outputs, 1)\n            \n                    # Compute accuracy\n                    total_validation_data = total_validation_data + labels.size(0)\n                    correct_predicted_validation_data = correct_predicted_validation_data + (predicted_by_model == labels).sum().item()\n            \n        \n                val_accuracy = 100 * correct_predicted_validation_data / total_validation_data\n                print(f'Epoch number {epoch+1}, Validation Accuracy: {val_accuracy:.2f}%')\n                wandb.log({'val_accuracy':val_accuracy})\n                wandb.log({'epoch':epoch+1})\n    \n\n        print('Finished Training') # Here training finished\n\nwandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing using best hyperparameter","metadata":{}},{"cell_type":"code","source":"# Importing essential libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Hyper-parameters\nnum_epochs = 5\nbatch_size = 32\nlearning_rate = 0.001\n\n# Define data transformation\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to a uniform size\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize images\n])\n\n# Load dataset\ndataset = ImageFolder(root='/kaggle/input/i-naturalist1/inaturalist_12K/train', transform=transform)\n\n# Split dataset into training and validation sets\ntraining_data_size = int(0.8 * len(dataset))\nvalidation_data_size = len(dataset) - training_data_size\ntraining_dataset, validation_dataset = random_split(dataset, [training_data_size, validation_data_size])\n\n# Create data loaders\ntraining_data_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\nvalidation_data_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\ntesting_dataset = ImageFolder(root='/kaggle/input/i-naturalist1/inaturalist_12K/val', transform=transform)\n\ntesting_data_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)\n\n#classes = dataset.classes\n#print(classes)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:44:49.039332Z","iopub.execute_input":"2024-04-07T19:44:49.039701Z","iopub.status.idle":"2024-04-07T19:44:51.820103Z","shell.execute_reply.started":"2024-04-07T19:44:49.039675Z","shell.execute_reply":"2024-04-07T19:44:51.819037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'name' : 'sweep test(test_data)1',\n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'kernel_size':{\n            'values': [[3,5,5,7,7]]\n        },\n        'dropout': {\n            'values': [0.3]\n        },\n        'activation': {\n            'values': ['GELU']\n        },\n        'batch_norm':{\n            'values': [False]\n        },\n        'filt_org':{\n            'values': [[32,64,128,256,512]]\n        },\n        'data_augment': {\n            'values': [False]\n        },\n        'num_dense':{\n            'values': [128]\n        }\n    }\n}\nsweep_id = wandb.sweep(sweep=sweep_config, project='Deep_Learning_Assignment2')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T19:46:04.948503Z","iopub.execute_input":"2024-04-07T19:46:04.948850Z","iopub.status.idle":"2024-04-07T19:46:05.223083Z","shell.execute_reply.started":"2024-04-07T19:46:04.948825Z","shell.execute_reply":"2024-04-07T19:46:05.222069Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Create sweep with ID: y48kx8hl\nSweep URL: https://wandb.ai/prabhat-kumar/Deep_Learning_Assignment2/sweeps/y48kx8hl\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    '''\n    WandB calls main function each time with differnet combination.\n\n    We can retrive the same and use the same values for our hypermeters.\n\n    '''\n\n\n    with wandb.init(entity = 'prabhat-kumar') as run:\n\n        run_name=\"-act_\"+wandb.config.activation+\"-ks\"+str(wandb.config.kernel_size)+'-nd'+str(wandb.config.num_dense)\n        wandb.run.name=run_name\n#         actv = get_activation(wandb.config.activation)\n        model = FlexibleCNN(num_classes=10, in_channels=3, conv_filters=wandb.config.filt_org, \n                 kernel_sizes=wandb.config.kernel_size, pool_sizes=[2, 2, 2, 2, 2], \n                 dense_units=wandb.config.num_dense, activation=wandb.config.activation).to(device)\n        \n        \n        # Loss and optimizer\n        criterion = nn.CrossEntropyLoss()\n        #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)   # Using Adam as an optimizer\n\n        # Training loop\n        for epoch in range(num_epochs):\n            #initializing the necessary values\n            total_training_data = 0\n            correct_predicted_train_data = 0\n            loss1 = 0\n            # Train the model\n            model.train()\n            for images, labels in training_data_loader:\n                # Moving images and labels to GPU\n                images = images.to(device)\n                labels = labels.to(device)\n        \n                # Forward pass\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n        \n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                loss1 = loss1 + loss.item()\n                \n                # Calculate the training accuracy\n                max_value, predicted_by_model = torch.max(outputs, 1)\n                total_training_data = total_training_data + labels.size(0)\n                correct_predicted_train_data = correct_predicted_train_data + (predicted_by_model == labels).sum().item()\n                \n            train_accuracy = 100*correct_predicted_train_data/total_training_data\n            print(f'Epoch number {epoch+1}, Training Accuracy: {train_accuracy:.2f}%') # Printing training accuracy\n            wandb.log({'train_loss': round(loss1 / len(training_data_loader), 2)})\n            wandb.log({'train_accuracy': round(train_accuracy, 2)})\n\n            #wandb.log({'train_loss':running_loss/len(train_loader)})\n            #wandb.log({'train_accuracy':train_accuracy})\n            \n        \n    \n            # Validate the model\n            model.eval()\n            with torch.no_grad():\n                # Initializing the values\n                correct_predicted_testing_data = 0\n                total_testing_data = 0\n                for images, labels in testing_data_loader:\n                    \n                    images = images.to(device)  # Moving images to GPU\n                    labels = labels.to(device)  # Moving labels to GPU\n            \n                    # Forward pass\n                    outputs = model(images)\n                    max_value, predicted_by_model = torch.max(outputs, 1)\n            \n                    # Compute accuracy\n                    total_testing_data = total_testing_data + labels.size(0)\n                    correct_predicted_testing_data = correct_predicted_testing_data + (predicted_by_model == labels).sum().item()\n            \n        \n                test_accuracy = 100 * correct_predicted_testing_data / total_testing_data\n                print(f'Epoch number {epoch+1}, Test Accuracy: {test_accuracy:.2f}%')\n                wandb.log({'test_accuracy':test_accuracy})\n                wandb.log({'epoch':epoch+1})\n    \n\n        print('Finished Training') # Here training finished\n\nwandb.agent(sweep_id, function=main,count=1) # calls main function for count number of times.\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}